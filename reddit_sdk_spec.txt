Spec Sheet — Intro Test Runner for Reddit “Hot” Posts (PRAW, Python)

Objective:
Build a small, reproducible probe that reads r/all (and optionally selected subreddits) using PRAW, stores raw/normalized data to JSON/NDJSON, stays under rate limits, and emits a Markdown report summarizing request costs (requests/post, comments/post, elapsed time, etc.).

⸻

1) Scope (MVP)
	•	Read top N “hot” posts from r/all (default N=100).
	•	For a configurable subset of those posts (default K=10), fetch and expand comments with a cap (e.g., replace_more(limit=5)).
	•	Record:
	•	Raw post fields + normalized projection.
	•	Raw comments + normalized projection (for the sampled K).
	•	Request/ratelimit telemetry and wall-clock timings.
	•	Respect rate limits with a local QPM governor and backoff on 429/5xx.
	•	Persist outputs to ./data/ and generate a ./reports/run_YYYYMMDD_HHMM.md summary.

Non-goals (for this first test): sentiment, OpenAI calls, historical storage, web UI.

⸻

2) Tech & Dependencies
	•	Language: Python 3.11
	•	Libraries:
	•	praw (Reddit client)
	•	prawcore (under the hood; used for request interception)
	•	httpx or requests (optional; only if you wrap your own requester)
	•	pydantic or typing (for schemas; optional)
	•	tqdm (optional progress bars)
	•	python-dotenv (optional env var loading)
	•	pytest (tests)
	•	Runtime: Local (CLI) or AWS Lambda later; MVP runs locally.

⸻

3) Configuration

Provide env vars or a config.yaml:

# config.yaml
reddit:
  client_id: ${REDDIT_CLIENT_ID}
  client_secret: ${REDDIT_CLIENT_SECRET}
  user_agent: yourapp:reddit-probe:v0.1 (by u/yourusername)

probe:
  subreddit: "all"              # "all" or a single subreddit (later: list)
  listing: "hot"                # "hot" | "top"
  time_filter: "day"            # only used if listing == "top"
  post_limit: 100               # N
  comment_sample: 10            # K
  comment_replace_more_limit: 5 # cap expansions; 0 means full (dangerous)
  qpm_cap: 90                   # client-side governor
  max_runtime_sec: 600          # safety cutoff
  raw_json: 1                   # pass raw_json=1 so text isn't HTML-escaped
  out_dir: "./data"
  reports_dir: "./reports"


⸻

4) Data Contracts

4.1 Normalized Post (JSONL record)

{
  "id": "t3_abcdef",
  "subreddit": "all",
  "title": "Post title",
  "selftext": "…",
  "url": "https://…",
  "domain": "example.com",
  "author": "username_or_null",
  "created_utc": "2025-09-29T01:23:45Z",
  "score": 1234,
  "num_comments": 456,
  "over_18": false,
  "upvote_ratio": 0.91,
  "permalink": "/r/…/comments/abcdef/title/",
  "retrieved_at": "2025-09-29T18:00:00Z"
}

4.2 Normalized Comment (JSONL record)

{
  "id": "t1_xyz",
  "link_id": "t3_abcdef",
  "parent_id": "t3_abcdef" ,
  "subreddit": "all",
  "author": "commenter_or_null",
  "body": "Comment text",
  "created_utc": "2025-09-29T01:45:00Z",
  "score": 42,
  "depth": 2,
  "retrieved_at": "2025-09-29T18:01:30Z"
}

4.3 Run Metrics (JSON)

{
  "run_id": "2025-09-29T18-02-00Z",
  "config": {
    "subreddit": "all",
    "listing": "hot",
    "post_limit": 100,
    "comment_sample": 10,
    "comment_replace_more_limit": 5,
    "qpm_cap": 90
  },
  "timing": {
    "started_at": "2025-09-29T18:00:00Z",
    "ended_at": "2025-09-29T18:10:00Z",
    "elapsed_sec": 600.2
  },
  "requests": {
    "total": 317,
    "per_post_stats": { "min": 1, "p50": 2, "p95": 8, "max": 40 },
    "comments_expansion_total": 285
  },
  "comments": {
    "total": 16432,
    "per_post_stats": { "min": 50, "p50": 800, "p95": 2500, "max": 4200 }
  },
  "ratelimit_windows": [
    {"used": 80, "remaining": 20, "reset_sec": 60}
  ],
  "notes": [
    "Hit 429 once while expanding comments; backed off 15s."
  ]
}


⸻

5) Output Files
	•	data/posts_YYYYMMDD_HHMM.jsonl — normalized posts (up to N lines).
	•	data/comments_YYYYMMDD_HHMM.jsonl — normalized comments for sampled K posts.
	•	data/run_metrics_YYYYMMDD_HHMM.json — metrics and telemetry.
	•	reports/run_YYYYMMDD_HHMM.md — human-friendly summary (see §8).

⸻

6) Implementation Plan

6.1 PRAW client initialization
	•	Create reddit_client.py:
	•	make_reddit(config) -> praw.Reddit (read-only).
	•	Ensure descriptive user agent string.
	•	Pass params={"raw_json": 1} to listing calls.

6.2 Rate-limit governor
	•	ratelimit.py:
	•	Token-bucket at qpm_cap (default 90).
	•	acquire() sleeps as needed to keep average <= cap.
	•	Backoff handler: on 429/5xx, exponential backoff + jitter; if Retry-After present, sleep that duration.

PRAW already avoids hammering, but we add a client-side governor to guarantee we never exceed caps when expanding big comment trees.

6.3 Response-header telemetry (optional but recommended)
	•	telemetry.py:
	•	Intercept underlying HTTP responses to capture X-Ratelimit-Used/Remaining/Reset.
	•	Two options:
	1.	Subclass prawcore.requestor.Requestor to wrap ._request and store headers centrally.
	2.	Use a tiny proxy (local or in-process) if subclassing feels intrusive (not required for MVP).
	•	Expose telemetry.record(headers, endpoint) → append to an in-memory list, flush summary to metrics JSON at end.

6.4 Listing fetch
	•	listings.py:
	•	iter_hot(subreddit: str, limit: int) -> Iterator[praw.models.Submission]
	•	For top, support time_filter (day/week/month).
	•	Normalize each submission to the post schema and write to posts_*.jsonl as we go (streaming).

6.5 Comment expansion
	•	comments.py:
	•	fetch_comments(submission_id: str, replace_more_limit: int) -> List[praw.models.Comment]
	•	Call submission.comments.replace_more(limit=replace_more_limit).
	•	Use the governor before operations that may fan out.
	•	Normalize and stream to comments_*.jsonl.

6.6 Orchestrator CLI
	•	probe.py:
	1.	Parse config / env.
	2.	Start timers; init telemetry & governor.
	3.	Fetch N posts from r/all (or configured sub).
	4.	Choose K post IDs to expand comments (e.g., highest num_comments).
	5.	Expand comments for those K with replace_more(limit=… ).
	6.	Write metrics JSON and render Markdown report.

⸻

7) Known Limits & Guardrails
	•	Listing per-page max: limit ≤ 100.
	•	Listing depth cap: cannot paginate beyond ~1000 items per listing (upstream API reality).
	•	For “hot 100”: 1 request (if limit=100).
	•	Rate limits (OAuth): budget for 100 QPM averaged over 10 minutes. Implement local cap (90–95 QPM) + backoff on 429/5xx.
	•	Comments cost: replace_more(limit=0) may cause dozens of calls on large threads; start with limit=5 and measure.
	•	User-Agent: must be descriptive.
	•	Deletion policy: if storing data beyond this test, purge items marked deleted within 48 hours (MVP can skip persistent storage; add note in report).

⸻

8) Markdown Report Template

reports/run_YYYYMMDD_HHMM.md

# Reddit Probe — r/${subreddit} (${listing}) — ${run_id}

**Config**
- posts: ${post_limit}, comment_sample: ${comment_sample}, replace_more_limit: ${comment_replace_more_limit}
- qpm_cap: ${qpm_cap}, raw_json: ${raw_json}

**Timing**
- started: ${started_at}, ended: ${ended_at}, elapsed: ${elapsed_sec}s

## Requests & Rate Limits
- total requests: **${requests.total}**
- per-post requests (comments-sampled only): min ${min}, p50 ${p50}, p95 ${p95}, max ${max}
- ratelimit windows observed: ${ratelimit_windows_count}
  - examples: `remaining=${rem}`, `reset=${reset_sec}s`

## Data Volume
- posts fetched: **${posts_count}**
- comments fetched (sample ${comment_sample}): **${comments_total}**
- comments per expanded post: min ${cmin}, p50 ${cp50}, p95 ${cp95}, max ${cmax}

## Notes & Incidents
- ${notes_list}

## Takeaways
- Is fetching hot ${post_limit} + comments(${comment_sample}, limit=${comment_replace_more_limit}) feasible under 100 QPM? ${feasible_yes_no}
- Recommended next tweak: ${suggestion}


⸻

9) Repo Layout

reddit-probe/
  config.yaml
  probe.py
  reddit_client.py
  listings.py
  comments.py
  ratelimit.py
  telemetry.py
  normalizers.py
  utils/time.py
  data/.gitkeep
  reports/.gitkeep
  requirements.txt
  README.md
  tests/
    test_normalizers.py
    test_ratelimit.py
    test_listings_unit.py

requirements.txt (minimal):

praw
python-dotenv
tqdm
pytest


⸻

10) Acceptance Criteria (for this first run)
	•	Running python probe.py produces:
	•	data/posts_*.jsonl with N normalized posts.
	•	data/comments_*.jsonl with comments for K posts.
	•	data/run_metrics_*.json with request counts, per-post stats, timings.
	•	reports/run_*.md summarizing results & feasibility.
	•	The process never exceeds the configured QPM cap and gracefully backs off on 429/5xx.
	•	The script terminates before max_runtime_sec (or logs that it hit the ceiling).

⸻

11) Stretch (Optional, not required now)
	•	Add CLI args (--subreddit, --posts, --sample, --replace-more-limit).
	•	Add a dry-run that only counts requests without writing files.
	•	Respect X-Ratelimit-* precisely via request interception and log a per-call timeline.

⸻

Hand this to Cursor to scaffold the files and wire up the CLI. You can run a probe within minutes and see real-world costs before you build the rest.